{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdfXZHpzN0TH"
   },
   "source": [
    "# Notebook 05: Comprehensive Evaluation Metrics\n",
    "## Benchmarking SAM & MedSAM Robustness under Noisy Abdominal CT Conditions\n",
    "\n",
    "**Author:** Hoang Le Chau        \n",
    "**Date:** January 2026\n",
    "\n",
    "---\n",
    "\n",
    "### Objective\n",
    "Calculate comprehensive evaluation metrics for all model predictions:\n",
    "1. Dice Coefficient - overlap-based metric\n",
    "2. IoU (Jaccard Index) - intersection over union\n",
    "3. Hausdorff Distance - boundary accuracy metric\n",
    "4. Precision and Recall - classification metrics\n",
    "5. Stability metrics - performance variance across noise levels\n",
    "\n",
    "These metrics enable quantitative comparison of model robustness."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FtSI18DLN0TI",
    "outputId": "9917d4d2-3eae-430e-9864-c489c224f476"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from scipy.spatial.distance import directed_hausdorff\n",
    "from scipy import ndimage\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-paper')\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q__sZcXuN0TJ"
   },
   "source": [
    "### 1. Mount Drive and Setup Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EpfD_CqjN0TK",
    "outputId": "8511eb4a-00e3-48a0-b9ba-07bec8971ca8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "Predictions path: /content/drive/MyDrive/Colab Notebooks/AIMA/sam_noisy/SAM_Robustness_Study/predictions\n",
      "Results will be saved to: /content/drive/MyDrive/Colab Notebooks/AIMA/sam_noisy/SAM_Robustness_Study/results\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "BASE_PATH = Path('/content/drive/MyDrive/Colab Notebooks/AIMA/sam_noisy/')\n",
    "OUTPUT_PATH = BASE_PATH / 'SAM_Robustness_Study'\n",
    "PREDICTIONS_PATH = OUTPUT_PATH / 'predictions'\n",
    "RESULTS_PATH = OUTPUT_PATH / 'results'\n",
    "RESULTS_PATH.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Predictions path: {PREDICTIONS_PATH}\")\n",
    "print(f\"Results will be saved to: {RESULTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOn-3ZEbN0TK"
   },
   "source": [
    "### 2. Evaluation Metrics Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B42St8xoN0TL",
    "outputId": "bfd08af0-6085-4438-bd0b-b4a2695eb309"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SegmentationMetrics class defined\n"
     ]
    }
   ],
   "source": [
    "class SegmentationMetrics:\n",
    "    \"\"\"\n",
    "    Class implementing comprehensive segmentation evaluation metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def dice_coefficient(pred, gt):\n",
    "        \"\"\"\n",
    "        Calculate Dice coefficient (F1 score).\n",
    "\n",
    "        Args:\n",
    "            pred: Prediction mask (binary)\n",
    "            gt: Ground truth mask (binary)\n",
    "\n",
    "        Returns:\n",
    "            Dice score [0, 1]\n",
    "        \"\"\"\n",
    "        pred_binary = (pred > 0).astype(np.float32)\n",
    "        gt_binary = (gt > 0).astype(np.float32)\n",
    "\n",
    "        intersection = (pred_binary * gt_binary).sum()\n",
    "        union = pred_binary.sum() + gt_binary.sum()\n",
    "\n",
    "        if union == 0:\n",
    "            return 1.0 if intersection == 0 else 0.0\n",
    "\n",
    "        return (2.0 * intersection) / union\n",
    "\n",
    "    @staticmethod\n",
    "    def iou(pred, gt):\n",
    "        \"\"\"\n",
    "        Calculate Intersection over Union (Jaccard Index).\n",
    "\n",
    "        Args:\n",
    "            pred: Prediction mask (binary)\n",
    "            gt: Ground truth mask (binary)\n",
    "\n",
    "        Returns:\n",
    "            IoU score [0, 1]\n",
    "        \"\"\"\n",
    "        pred_binary = (pred > 0).astype(np.float32)\n",
    "        gt_binary = (gt > 0).astype(np.float32)\n",
    "\n",
    "        intersection = (pred_binary * gt_binary).sum()\n",
    "        union = pred_binary.sum() + gt_binary.sum() - intersection\n",
    "\n",
    "        if union == 0:\n",
    "            return 1.0 if intersection == 0 else 0.0\n",
    "\n",
    "        return intersection / union\n",
    "\n",
    "    @staticmethod\n",
    "    def precision_recall(pred, gt):\n",
    "        \"\"\"\n",
    "        Calculate precision and recall.\n",
    "\n",
    "        Args:\n",
    "            pred: Prediction mask (binary)\n",
    "            gt: Ground truth mask (binary)\n",
    "\n",
    "        Returns:\n",
    "            Tuple of (precision, recall)\n",
    "        \"\"\"\n",
    "        pred_binary = (pred > 0).astype(np.float32)\n",
    "        gt_binary = (gt > 0).astype(np.float32)\n",
    "\n",
    "        tp = (pred_binary * gt_binary).sum()\n",
    "        fp = ((pred_binary - gt_binary) > 0).sum()\n",
    "        fn = ((gt_binary - pred_binary) > 0).sum()\n",
    "\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0.0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "\n",
    "        return precision, recall\n",
    "\n",
    "    @staticmethod\n",
    "    def hausdorff_distance(pred, gt):\n",
    "        \"\"\"\n",
    "        Calculate Hausdorff distance between boundaries.\n",
    "\n",
    "        Args:\n",
    "            pred: Prediction mask (binary)\n",
    "            gt: Ground truth mask (binary)\n",
    "\n",
    "        Returns:\n",
    "            Hausdorff distance (pixels)\n",
    "        \"\"\"\n",
    "        pred_binary = (pred > 0).astype(np.uint8)\n",
    "        gt_binary = (gt > 0).astype(np.uint8)\n",
    "\n",
    "        if pred_binary.sum() == 0 or gt_binary.sum() == 0:\n",
    "            return np.inf\n",
    "\n",
    "        pred_boundary = pred_binary - ndimage.binary_erosion(pred_binary)\n",
    "        gt_boundary = gt_binary - ndimage.binary_erosion(gt_binary)\n",
    "\n",
    "        pred_points = np.argwhere(pred_boundary > 0)\n",
    "        gt_points = np.argwhere(gt_boundary > 0)\n",
    "\n",
    "        if len(pred_points) == 0 or len(gt_points) == 0:\n",
    "            return np.inf\n",
    "\n",
    "        hd1 = directed_hausdorff(pred_points, gt_points)[0]\n",
    "        hd2 = directed_hausdorff(gt_points, pred_points)[0]\n",
    "\n",
    "        return max(hd1, hd2)\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_all_metrics(pred, gt):\n",
    "        \"\"\"\n",
    "        Compute all metrics for a single prediction-GT pair.\n",
    "\n",
    "        Args:\n",
    "            pred: Prediction mask\n",
    "            gt: Ground truth mask\n",
    "\n",
    "        Returns:\n",
    "            Dictionary of metrics\n",
    "        \"\"\"\n",
    "        dice = SegmentationMetrics.dice_coefficient(pred, gt)\n",
    "        iou_score = SegmentationMetrics.iou(pred, gt)\n",
    "        precision, recall = SegmentationMetrics.precision_recall(pred, gt)\n",
    "        hausdorff = SegmentationMetrics.hausdorff_distance(pred, gt)\n",
    "\n",
    "        return {\n",
    "            'dice': dice,\n",
    "            'iou': iou_score,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'hausdorff': hausdorff\n",
    "        }\n",
    "\n",
    "print(\"SegmentationMetrics class defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jv69bK7vN0TL"
   },
   "source": [
    "### 3. Batch Evaluation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XOMgUVqbN0TM",
    "outputId": "18669ee7-ce32-4272-91f8-8194c3e3480f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "def evaluate_predictions(pred_path, gt_path):\n",
    "    \"\"\"\n",
    "    Evaluate all predictions in a directory.\n",
    "\n",
    "    Args:\n",
    "        pred_path: Path to predictions.npy\n",
    "        gt_path: Path to ground_truth.npy\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with metrics for each image\n",
    "    \"\"\"\n",
    "    predictions = np.load(pred_path)\n",
    "    ground_truths = np.load(gt_path)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for idx in range(len(predictions)):\n",
    "        metrics = SegmentationMetrics.compute_all_metrics(\n",
    "            predictions[idx],\n",
    "            ground_truths[idx]\n",
    "        )\n",
    "        metrics['image_idx'] = idx\n",
    "        results.append(metrics)\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "def get_all_prediction_paths(predictions_base_path, dataset_name):\n",
    "    \"\"\"\n",
    "    Get all prediction paths for a dataset.\n",
    "\n",
    "    Args:\n",
    "        predictions_base_path: Base predictions directory\n",
    "        dataset_name: Name of dataset\n",
    "\n",
    "    Returns:\n",
    "        List of tuples (variant_name, model_name, pred_path, gt_path)\n",
    "    \"\"\"\n",
    "    dataset_path = predictions_base_path / dataset_name\n",
    "    paths = []\n",
    "\n",
    "    for variant_dir in sorted(dataset_path.iterdir()):\n",
    "        if variant_dir.is_dir():\n",
    "            for model_dir in sorted(variant_dir.iterdir()):\n",
    "                if model_dir.is_dir():\n",
    "                    pred_path = model_dir / 'predictions.npy'\n",
    "                    gt_path = model_dir / 'ground_truth.npy'\n",
    "\n",
    "                    if pred_path.exists() and gt_path.exists():\n",
    "                        paths.append((\n",
    "                            variant_dir.name,\n",
    "                            model_dir.name,\n",
    "                            pred_path,\n",
    "                            gt_path\n",
    "                        ))\n",
    "\n",
    "    return paths\n",
    "\n",
    "print(\"Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RVvI9C35N0TM"
   },
   "source": [
    "### 4. Evaluate All Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "X-Xu6rR0N0TN",
    "outputId": "6b89ca41-b520-4dd3-bb16-a2979fc85cef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating Liver dataset...\n",
      "\n",
      "======================================================================\n",
      "Evaluating LIVER Dataset\n",
      "======================================================================\n",
      "\n",
      "Found 38 prediction sets to evaluate\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 38/38 [01:18<00:00,  2.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed results saved to /content/drive/MyDrive/Colab Notebooks/AIMA/sam_noisy/SAM_Robustness_Study/results/liver_detailed_metrics.csv\n",
      "\n",
      "Evaluating Spleen dataset...\n",
      "\n",
      "======================================================================\n",
      "Evaluating SPLEEN Dataset\n",
      "======================================================================\n",
      "\n",
      "Found 38 prediction sets to evaluate\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|██████████| 38/38 [01:13<00:00,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed results saved to /content/drive/MyDrive/Colab Notebooks/AIMA/sam_noisy/SAM_Robustness_Study/results/spleen_detailed_metrics.csv\n",
      "\n",
      "======================================================================\n",
      "EVALUATION COMPLETE\n",
      "======================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def evaluate_dataset(dataset_name, predictions_base_path, results_save_path):\n",
    "    \"\"\"\n",
    "    Evaluate all predictions for a dataset.\n",
    "\n",
    "    Args:\n",
    "        dataset_name: Name of dataset\n",
    "        predictions_base_path: Base predictions directory\n",
    "        results_save_path: Path to save results\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with all results\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"Evaluating {dataset_name.upper()} Dataset\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "\n",
    "    paths = get_all_prediction_paths(predictions_base_path, dataset_name)\n",
    "    print(f\"Found {len(paths)} prediction sets to evaluate\\n\")\n",
    "\n",
    "    all_results = []\n",
    "\n",
    "    for variant_name, model_name, pred_path, gt_path in tqdm(paths, desc=\"Evaluating\"):\n",
    "        metrics_df = evaluate_predictions(pred_path, gt_path)\n",
    "\n",
    "        metrics_df['variant'] = variant_name\n",
    "        metrics_df['model'] = model_name.upper()\n",
    "        metrics_df['dataset'] = dataset_name\n",
    "\n",
    "        noise_type = '_'.join(variant_name.split('_')[:-1]) if variant_name != 'clean' else 'clean'\n",
    "        intensity = variant_name.split('_')[-1] if variant_name != 'clean' else 'clean'\n",
    "\n",
    "        metrics_df['noise_type'] = noise_type\n",
    "        metrics_df['intensity'] = intensity\n",
    "\n",
    "        all_results.append(metrics_df)\n",
    "\n",
    "    full_results = pd.concat(all_results, ignore_index=True)\n",
    "\n",
    "    save_path = results_save_path / f\"{dataset_name}_detailed_metrics.csv\"\n",
    "    full_results.to_csv(save_path, index=False)\n",
    "    print(f\"\\nDetailed results saved to {save_path}\")\n",
    "\n",
    "    return full_results\n",
    "\n",
    "print(\"Evaluating Liver dataset...\")\n",
    "liver_results = evaluate_dataset('liver', PREDICTIONS_PATH, RESULTS_PATH)\n",
    "\n",
    "print(\"\\nEvaluating Spleen dataset...\")\n",
    "spleen_results = evaluate_dataset('spleen', PREDICTIONS_PATH, RESULTS_PATH)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION COMPLETE\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43qh6JWeN0TN"
   },
   "source": [
    "### 5. Aggregate Statistics by Noise Type and Intensity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zveyCB1qN0TN",
    "outputId": "6370fe67-5594-40f3-b3a4-b7cf250039c8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Aggregate statistics computed and saved\n",
      "\n",
      "Liver aggregate shape: (38, 20)\n",
      "Spleen aggregate shape: (38, 20)\n"
     ]
    }
   ],
   "source": [
    "def compute_aggregate_statistics(results_df):\n",
    "    \"\"\"\n",
    "    Compute aggregate statistics grouped by variant and model.\n",
    "\n",
    "    Args:\n",
    "        results_df: DataFrame with detailed results\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with aggregate statistics\n",
    "    \"\"\"\n",
    "    grouped = results_df.groupby(['dataset', 'variant', 'model', 'noise_type', 'intensity'])\n",
    "\n",
    "    agg_stats = grouped.agg({\n",
    "        'dice': ['mean', 'std', 'min', 'max'],\n",
    "        'iou': ['mean', 'std', 'min', 'max'],\n",
    "        'precision': ['mean', 'std'],\n",
    "        'recall': ['mean', 'std'],\n",
    "        'hausdorff': ['mean', 'std', 'median']\n",
    "    }).reset_index()\n",
    "\n",
    "    agg_stats.columns = ['_'.join(col).strip('_') for col in agg_stats.columns.values]\n",
    "\n",
    "    return agg_stats\n",
    "\n",
    "liver_agg = compute_aggregate_statistics(liver_results)\n",
    "spleen_agg = compute_aggregate_statistics(spleen_results)\n",
    "\n",
    "liver_agg.to_csv(RESULTS_PATH / 'liver_aggregate_metrics.csv', index=False)\n",
    "spleen_agg.to_csv(RESULTS_PATH / 'spleen_aggregate_metrics.csv', index=False)\n",
    "\n",
    "print(\"\\nAggregate statistics computed and saved\")\n",
    "print(f\"\\nLiver aggregate shape: {liver_agg.shape}\")\n",
    "print(f\"Spleen aggregate shape: {spleen_agg.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4IYgkYGN0TO"
   },
   "source": [
    "### 6. Performance Summary Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VgivDjdqN0TO",
    "outputId": "396e66bf-88cd-4b28-9053-6f9d441e16ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "LIVER DATASET - PERFORMANCE SUMMARY (Top 10)\n",
      "================================================================================\n",
      "                     variant  model  dice_mean  iou_mean  precision_mean  recall_mean  hausdorff_mean\n",
      "                       clean MEDSAM     0.0000    0.0000          0.0000       0.0000             inf\n",
      "                       clean    SAM     0.1313    0.0739          0.0739       1.0000        375.1101\n",
      "               gaussian_mild MEDSAM     0.0692    0.0391          0.0500       0.1463        312.8318\n",
      "               gaussian_mild    SAM     0.1394    0.0793          0.0793       1.0000        378.5870\n",
      "           gaussian_moderate MEDSAM     0.0803    0.0464          0.0582       0.1495        314.5615\n",
      "           gaussian_moderate    SAM     0.1343    0.0764          0.0764       1.0000        386.2715\n",
      "             gaussian_severe MEDSAM     0.0394    0.0216          0.0285       0.0750        319.6416\n",
      "             gaussian_severe    SAM     0.1312    0.0743          0.0743       1.0000        397.9276\n",
      "intensity_inhomogeneity_mild MEDSAM     0.0597    0.0347          0.0452       0.1055        314.7656\n",
      "intensity_inhomogeneity_mild    SAM     0.1315    0.0743          0.0743       1.0000        374.7545\n",
      "\n",
      "================================================================================\n",
      "SPLEEN DATASET - PERFORMANCE SUMMARY (Top 10)\n",
      "================================================================================\n",
      "                     variant  model  dice_mean  iou_mean  precision_mean  recall_mean  hausdorff_mean\n",
      "                       clean MEDSAM     0.0000    0.0000          0.0000       0.0000             inf\n",
      "                       clean    SAM     0.0396    0.0203          0.0203       1.0000        417.5114\n",
      "               gaussian_mild MEDSAM     0.0610    0.0326          0.0329       0.4736        359.2506\n",
      "               gaussian_mild    SAM     0.0440    0.0226          0.0226       1.0000        413.1458\n",
      "           gaussian_moderate MEDSAM     0.0650    0.0347          0.0350       0.5148        361.7848\n",
      "           gaussian_moderate    SAM     0.0426    0.0219          0.0219       0.9800        412.1329\n",
      "             gaussian_severe MEDSAM     0.0644    0.0345          0.0348       0.5182        359.7597\n",
      "             gaussian_severe    SAM     0.0414    0.0213          0.0213       1.0000        415.2366\n",
      "intensity_inhomogeneity_mild MEDSAM     0.0563    0.0303          0.0307       0.4068        364.3566\n",
      "intensity_inhomogeneity_mild    SAM     0.0391    0.0200          0.0200       1.0000        423.9549\n"
     ]
    }
   ],
   "source": [
    "def create_performance_summary(agg_df, dataset_name):\n",
    "    \"\"\"\n",
    "    Create performance summary table.\n",
    "\n",
    "    Args:\n",
    "        agg_df: Aggregate statistics DataFrame\n",
    "        dataset_name: Name of dataset\n",
    "\n",
    "    Returns:\n",
    "        Summary DataFrame\n",
    "    \"\"\"\n",
    "    summary_cols = ['variant', 'model', 'dice_mean', 'iou_mean',\n",
    "                   'precision_mean', 'recall_mean', 'hausdorff_mean']\n",
    "    summary = agg_df[summary_cols].copy()\n",
    "\n",
    "    summary = summary.round(4)\n",
    "\n",
    "    return summary\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LIVER DATASET - PERFORMANCE SUMMARY (Top 10)\")\n",
    "print(\"=\"*80)\n",
    "liver_summary = create_performance_summary(liver_agg, 'Liver')\n",
    "print(liver_summary.head(10).to_string(index=False))\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SPLEEN DATASET - PERFORMANCE SUMMARY (Top 10)\")\n",
    "print(\"=\"*80)\n",
    "spleen_summary = create_performance_summary(spleen_agg, 'Spleen')\n",
    "print(spleen_summary.head(10).to_string(index=False))\n",
    "\n",
    "liver_summary.to_csv(RESULTS_PATH / 'liver_summary.csv', index=False)\n",
    "spleen_summary.to_csv(RESULTS_PATH / 'spleen_summary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KBlFzV-fN0TP"
   },
   "source": [
    "### 7. Model Comparison (Clean vs Noisy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Vs2NDq89N0TP",
    "outputId": "6ad7a113-0b31-44dc-82f5-e6fba04109c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CLEAN VS NOISY COMPARISON\n",
      "================================================================================\n",
      "\n",
      "Liver Dataset:\n",
      " model  dice_clean  iou_clean  hausdorff_clean  dice_noisy  iou_noisy  hausdorff_noisy  dice_degradation  iou_degradation  hausdorff_increase\n",
      "MEDSAM      0.0000     0.0000              inf      0.0586     0.0333         314.6080           -0.0586          -0.0333                -inf\n",
      "   SAM      0.1313     0.0739         375.1101      0.1320     0.0745         380.7734           -0.0007          -0.0006              5.6634\n",
      "\n",
      "Spleen Dataset:\n",
      " model  dice_clean  iou_clean  hausdorff_clean  dice_noisy  iou_noisy  hausdorff_noisy  dice_degradation  iou_degradation  hausdorff_increase\n",
      "MEDSAM      0.0000     0.0000              inf      0.0597     0.0320         357.2129           -0.0597          -0.0320                -inf\n",
      "   SAM      0.0396     0.0203         417.5114      0.0404     0.0207         416.5814           -0.0008          -0.0004             -0.9299\n"
     ]
    }
   ],
   "source": [
    "def compare_clean_vs_noisy(results_df, dataset_name):\n",
    "    \"\"\"\n",
    "    Compare performance on clean vs noisy data.\n",
    "\n",
    "    Args:\n",
    "        results_df: Results DataFrame\n",
    "        dataset_name: Name of dataset\n",
    "\n",
    "    Returns:\n",
    "        Comparison DataFrame\n",
    "    \"\"\"\n",
    "    clean_results = results_df[results_df['variant'] == 'clean'].groupby('model').agg({\n",
    "        'dice': 'mean',\n",
    "        'iou': 'mean',\n",
    "        'hausdorff': 'mean'\n",
    "    }).reset_index()\n",
    "    clean_results.columns = ['model', 'dice_clean', 'iou_clean', 'hausdorff_clean']\n",
    "\n",
    "    noisy_results = results_df[results_df['variant'] != 'clean'].groupby('model').agg({\n",
    "        'dice': 'mean',\n",
    "        'iou': 'mean',\n",
    "        'hausdorff': 'mean'\n",
    "    }).reset_index()\n",
    "    noisy_results.columns = ['model', 'dice_noisy', 'iou_noisy', 'hausdorff_noisy']\n",
    "\n",
    "    comparison = pd.merge(clean_results, noisy_results, on='model')\n",
    "\n",
    "    comparison['dice_degradation'] = comparison['dice_clean'] - comparison['dice_noisy']\n",
    "    comparison['iou_degradation'] = comparison['iou_clean'] - comparison['iou_noisy']\n",
    "    comparison['hausdorff_increase'] = comparison['hausdorff_noisy'] - comparison['hausdorff_clean']\n",
    "\n",
    "    return comparison.round(4)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CLEAN VS NOISY COMPARISON\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nLiver Dataset:\")\n",
    "liver_comparison = compare_clean_vs_noisy(liver_results, 'Liver')\n",
    "print(liver_comparison.to_string(index=False))\n",
    "\n",
    "print(\"\\nSpleen Dataset:\")\n",
    "spleen_comparison = compare_clean_vs_noisy(spleen_results, 'Spleen')\n",
    "print(spleen_comparison.to_string(index=False))\n",
    "\n",
    "liver_comparison.to_csv(RESULTS_PATH / 'liver_clean_vs_noisy.csv', index=False)\n",
    "spleen_comparison.to_csv(RESULTS_PATH / 'spleen_clean_vs_noisy.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YOfDFi6vN0TP"
   },
   "source": [
    "### 8. Robustness Ranking by Noise Type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qIJBrOYXN0TP",
    "outputId": "37adebc1-8aca-432d-80db-d7768e6be130"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "NOISE IMPACT RANKING (Most to Least Harmful)\n",
      "======================================================================\n",
      "\n",
      "Liver Dataset:\n",
      "             noise_type   dice    iou  dice_drop\n",
      "            motion_blur 0.0867 0.0490    -0.0211\n",
      "                poisson 0.0907 0.0510    -0.0250\n",
      "intensity_inhomogeneity 0.0944 0.0535    -0.0288\n",
      "           low_contrast 0.0980 0.0552    -0.0323\n",
      "               gaussian 0.0990 0.0562    -0.0334\n",
      "            salt_pepper 0.1030 0.0587    -0.0373\n",
      "\n",
      "Spleen Dataset:\n",
      "             noise_type   dice    iou  dice_drop\n",
      "           low_contrast 0.0462 0.0243    -0.0264\n",
      "            motion_blur 0.0467 0.0247    -0.0269\n",
      "                poisson 0.0480 0.0251    -0.0282\n",
      "intensity_inhomogeneity 0.0490 0.0258    -0.0292\n",
      "               gaussian 0.0531 0.0279    -0.0333\n",
      "            salt_pepper 0.0574 0.0303    -0.0376\n"
     ]
    }
   ],
   "source": [
    "def rank_noise_impact(results_df, dataset_name):\n",
    "    \"\"\"\n",
    "    Rank noise types by their impact on performance.\n",
    "\n",
    "    Args:\n",
    "        results_df: Results DataFrame\n",
    "        dataset_name: Name of dataset\n",
    "\n",
    "    Returns:\n",
    "        Ranked DataFrame\n",
    "    \"\"\"\n",
    "    clean_dice = results_df[results_df['variant'] == 'clean']['dice'].mean()\n",
    "\n",
    "    noisy_by_type = results_df[results_df['variant'] != 'clean'].groupby('noise_type').agg({\n",
    "        'dice': 'mean',\n",
    "        'iou': 'mean'\n",
    "    }).reset_index()\n",
    "\n",
    "    noisy_by_type['dice_drop'] = clean_dice - noisy_by_type['dice']\n",
    "    noisy_by_type = noisy_by_type.sort_values('dice_drop', ascending=False)\n",
    "\n",
    "    return noisy_by_type.round(4)\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"NOISE IMPACT RANKING (Most to Least Harmful)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\nLiver Dataset:\")\n",
    "liver_noise_rank = rank_noise_impact(liver_results, 'Liver')\n",
    "print(liver_noise_rank.to_string(index=False))\n",
    "\n",
    "print(\"\\nSpleen Dataset:\")\n",
    "spleen_noise_rank = rank_noise_impact(spleen_results, 'Spleen')\n",
    "print(spleen_noise_rank.to_string(index=False))\n",
    "\n",
    "liver_noise_rank.to_csv(RESULTS_PATH / 'liver_noise_impact_ranking.csv', index=False)\n",
    "spleen_noise_rank.to_csv(RESULTS_PATH / 'spleen_noise_impact_ranking.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "py8vTykeN0TQ"
   },
   "source": [
    "### 9. Final Summary Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RwY1hnajN0TQ",
    "outputId": "71ddb693-e8c3-4299-8862-600e19a749b6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "EVALUATION METRICS SUMMARY\n",
      "================================================================================\n",
      "Total Evaluations: 3800\n",
      "Liver Samples: 1900\n",
      "Spleen Samples: 1900\n",
      "Metrics Computed: ['Dice', 'IoU', 'Precision', 'Recall', 'Hausdorff Distance']\n",
      "Models Evaluated: ['MEDSAM', 'SAM']\n",
      "Noise Types Evaluated: ['clean', 'gaussian', 'intensity_inhomogeneity', 'low_contrast', 'motion_blur', 'poisson', 'salt_pepper']\n",
      "\n",
      "Files Saved:\n",
      "  - liver_detailed_metrics.csv\n",
      "  - spleen_detailed_metrics.csv\n",
      "  - liver_aggregate_metrics.csv\n",
      "  - spleen_aggregate_metrics.csv\n",
      "  - liver_summary.csv\n",
      "  - spleen_summary.csv\n",
      "  - liver_clean_vs_noisy.csv\n",
      "  - spleen_clean_vs_noisy.csv\n",
      "  - liver_noise_impact_ranking.csv\n",
      "  - spleen_noise_impact_ranking.csv\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EVALUATION METRICS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "summary_info = {\n",
    "    'Total Evaluations': len(liver_results) + len(spleen_results),\n",
    "    'Liver Samples': len(liver_results),\n",
    "    'Spleen Samples': len(spleen_results),\n",
    "    'Metrics Computed': ['Dice', 'IoU', 'Precision', 'Recall', 'Hausdorff Distance'],\n",
    "    'Models Evaluated': liver_results['model'].unique().tolist(),\n",
    "    'Noise Types Evaluated': liver_results['noise_type'].unique().tolist()\n",
    "}\n",
    "\n",
    "for key, value in summary_info.items():\n",
    "    print(f\"{key}: {value}\")\n",
    "\n",
    "print(\"\\nFiles Saved:\")\n",
    "saved_files = [\n",
    "    'liver_detailed_metrics.csv',\n",
    "    'spleen_detailed_metrics.csv',\n",
    "    'liver_aggregate_metrics.csv',\n",
    "    'spleen_aggregate_metrics.csv',\n",
    "    'liver_summary.csv',\n",
    "    'spleen_summary.csv',\n",
    "    'liver_clean_vs_noisy.csv',\n",
    "    'spleen_clean_vs_noisy.csv',\n",
    "    'liver_noise_impact_ranking.csv',\n",
    "    'spleen_noise_impact_ranking.csv'\n",
    "]\n",
    "\n",
    "for filename in saved_files:\n",
    "    print(f\"  - {filename}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
